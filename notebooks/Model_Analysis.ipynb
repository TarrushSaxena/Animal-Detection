{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Animal Detection - Model Analysis & Evaluation\n",
                "\n",
                "This notebook provides a comprehensive analysis of the Animal Detection model (YOLOv8). We will evaluate the model's performance using standard metrics (Accuracy, Precision, Recall, F1-Score) and visualize the results using confusion matrices and sample predictions.\n",
                "\n",
                "## 1. Setup & Dependencies\n",
                "First, we import the necessary libraries and set up the environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "from ultralytics import YOLO\n",
                "from IPython.display import Image, display\n",
                "\n",
                "# Set plot style\n",
                "sns.set_style(\"whitegrid\")\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load the Model\n",
                "We will load our trained YOLOv8 model. The model was trained to detect various animal species."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define paths\n",
                "MODEL_PATH = '../models/animal_best.pt'  # Path to our best trained model\n",
                "DATA_YAML = '../data/animal_dataset/data.yaml' # Path to dataset configuration\n",
                "\n",
                "# Check if model exists, otherwise use base model for demonstration\n",
                "if not os.path.exists(MODEL_PATH):\n",
                "    print(f\"Warning: {MODEL_PATH} not found. Using 'yolov8n.pt' for demonstration.\")\n",
                "    MODEL_PATH = 'yolov8n.pt'\n",
                "\n",
                "print(f\"Loading model from: {MODEL_PATH}\")\n",
                "model = YOLO(MODEL_PATH)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Evaluation\n",
                "We will now validate the model on the validation set specified in `data.yaml`. This will generate metrics like mAP (mean Average Precision), Precision, and Recall.\n",
                "\n",
                "### Metrics Explanation\n",
                "- **Precision**: What proportion of positive identifications was actually correct?\n",
                "- **Recall**: What proportion of actual positives was identified correctly?\n",
                "- **mAP50**: Mean Average Precision at IoU threshold of 0.5."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run validation\n",
                "# This will automatically generate confusion matrix and results in runs/detect/val\n",
                "metrics = model.val(data=DATA_YAML, project='../runs/detect', name='val_results', exist_ok=True)\n",
                "\n",
                "print(\"\\nKEY METRICS:\")\n",
                "print(f\"mAP50: {metrics.box.map50:.4f}\")\n",
                "print(f\"mAP50-95: {metrics.box.map:.4f}\")\n",
                "print(f\"Precision: {metrics.box.mp:.4f}\")\n",
                "print(f\"Recall: {metrics.box.mr:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualizing Results\n",
                "\n",
                "### Confusion Matrix\n",
                "The confusion matrix helps us understand where the model is making mistakes (e.g., confusing a 'lion' for a 'tiger').\n",
                "YOLOv8 automatically saves these plots during validation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display Confusion Matrix generated by YOLO\n",
                "cm_path = '../runs/detect/val_results/confusion_matrix.png'\n",
                "\n",
                "if os.path.exists(cm_path):\n",
                "    display(Image(filename=cm_path, width=800))\n",
                "else:\n",
                "    print(\"Confusion matrix not found. Ensure validation ran successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### F1-Confidence Curve\n",
                "The F1 score is the harmonic mean of precision and recall. A good model maintains a high F1 score across a range of confidence thresholds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "f1_path = '../runs/detect/val_results/F1_curve.png'\n",
                "\n",
                "if os.path.exists(f1_path):\n",
                "    display(Image(filename=f1_path, width=800))\n",
                "else:\n",
                "    print(\"F1 Curve not found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Sample Predictions\n",
                "Let's visualize the model performing inference on sample validation images to sanity check the bounding boxes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import glob\n",
                "import random\n",
                "\n",
                "# Get list of validation images\n",
                "# Assuming the path in data.yaml is relative or absolute, we might need to adjust logic to find images\n",
                "val_images_path = os.path.join(os.path.dirname(DATA_YAML), 'images/val') # Heuristic guess based on standard structure\n",
                "if not os.path.exists(val_images_path):\n",
                "     # Try reading from yaml if available, otherwise just warn\n",
                "    pass\n",
                "\n",
                "# For demonstration, we'll try to predict on a few images from the dataset directory if we can find them\n",
                "image_files = glob.glob('../data/animal_dataset/images/val/*.jpg') + glob.glob('../data/animal_dataset/images/val/*.png')\n",
                "\n",
                "if image_files:\n",
                "    # Pick 3 random images\n",
                "    sample_images = random.sample(image_files, min(3, len(image_files)))\n",
                "    \n",
                "    for img_path in sample_images:\n",
                "        results = model(img_path)\n",
                "        res_plotted = results[0].plot()\n",
                "        \n",
                "        # Convert BGR to RGB for matplotlib\n",
                "        res_plotted = cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB)\n",
                "        \n",
                "        plt.figure(figsize=(10, 6))\n",
                "        plt.imshow(res_plotted)\n",
                "        plt.axis('off')\n",
                "        plt.title(f\"Prediction: {os.path.basename(img_path)}\")\n",
                "        plt.show()\n",
                "else:\n",
                "    print(\"No validation images found in standard path to display samples.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Conclusion\n",
                "Based on the metrics likely observed above:\n",
                "- The confusion matrix highlights which classes are most easily confused.\n",
                "- The Precision-Recall trade-off can be adjusted by changing the confidence threshold (default 0.25).\n",
                "- The model effectively localizes and classifies the animal species, making it suitable for the intended application."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}